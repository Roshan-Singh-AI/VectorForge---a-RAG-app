# VectorForge ğŸ§©

A **Streamlit-based conversational AI app** powered by `llama-3.3-70b-versatile`.  
This project demonstrates a simple chat interface where user inputs are processed by an LLM, and responses are streamed back in real time.

---

## ğŸŒ Live Demo

You can try the deployed app here:  
ğŸ‘‰ [VectorForge on Streamlit](https://vectorforge.streamlit.app/)

---
## ğŸš€ Features

- **Streamlit UI** â€“ Clean and interactive chat interface.
- **Chat Memory** â€“ Maintains conversation history across user and assistant turns.
- **Streaming Responses** â€“ AI responses are streamed in real time for a smoother experience.
- **Custom Model Integration** â€“ Uses `llama-3.3-70b-versatile` (configurable from `src/config`).

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app.py               # Main Streamlit app (entry point)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ config.py        # LLM model configuration (groq_llm_model)
â””â”€â”€ README.md            # Project documentation
```

---

## âš™ï¸ Installation & Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/Roshan-Singh-AI/VectorForge---a-RAG-app/
   cd vectorforge
   ```

2. **Create a virtual environment (optional but recommended)**
   ```bash
   python -m venv venv
   source venv/bin/activate   # On macOS/Linux
   venv\Scripts\activate      # On Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure your LLM**  
   Create an .env file with the following values - 
   ```
   groq_api_key = "<groq_api_key>"
   groq_llama_model = "llama-3.3-70b-versatile"
   ```

---

## â–¶ï¸ Running the App

Start the Streamlit server:

```bash
streamlit run app.py
```

The app will be available at:  
ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ§‘â€ğŸ’» Usage

- Type your prompt into the chat input field.
- The assistant will generate a **streamed response** in real time.
- Conversation history is maintained across turns.

---

## ğŸ› ï¸ Tech Stack

- [Streamlit](https://streamlit.io/) â€“ Web app framework
- [Python](https://www.python.org/) â€“ Core language
- `groq_llm_model` â€“ Custom LLM integration (replaceable with any API/model)

---

## ğŸ¤ Contributing

Pull requests are welcome!  
For major changes, please open an issue first to discuss what you would like to change.

---

## ğŸ“œ License

This project is licensed under the MIT License. See `LICENSE` for details.
